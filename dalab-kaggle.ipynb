{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":118082,"databundleVersionId":14294892,"sourceType":"competition"},{"sourceId":13739028,"sourceType":"datasetVersion","datasetId":8741855},{"sourceId":13741507,"sourceType":"datasetVersion","datasetId":8743424},{"sourceId":13773059,"sourceType":"datasetVersion","datasetId":8765955},{"sourceId":13787097,"sourceType":"datasetVersion","datasetId":8776806}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport pickle\nimport os\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom collections import Counter\nimport xgboost as xgb\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Create output directory\noutput_dir = Path('/kaggle/working/output')\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Load datasets\nwith open('/kaggle/input/da5401-2025-data-challenge/train_data.json', 'r') as f:\n    train_data = json.load(f)\n\nwith open('/kaggle/input/da5401-2025-data-challenge/test_data.json', 'r') as f:\n    test_data = json.load(f)\n\nwith open('/kaggle/input/da5401-2025-data-challenge/metric_names.json', 'r') as f:\n    metric_names_list = json.load(f)\n\nmetric_embeddings = np.load('/kaggle/input/da5401-2025-data-challenge/metric_name_embeddings.npy')\nsample_submission = pd.read_csv('//kaggle/input/da5401-2025-data-challenge/sample_submission.csv')\n\n# Convert to DataFrames\ntrain_df = pd.DataFrame(train_data)\ntest_df = pd.DataFrame(test_data)\n\nprint(f\"✓ Training samples: {len(train_df)}\")\nprint(f\"✓ Test samples: {len(test_df)}\")\nprint(f\"✓ Metric embeddings shape: {metric_embeddings.shape}\")\nprint(f\"✓ Unique metrics: {train_df['metric_name'].nunique()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T13:04:23.889479Z","iopub.execute_input":"2025-11-21T13:04:23.890261Z","iopub.status.idle":"2025-11-21T13:04:24.080261Z","shell.execute_reply.started":"2025-11-21T13:04:23.890234Z","shell.execute_reply":"2025-11-21T13:04:24.079482Z"}},"outputs":[{"name":"stdout","text":"✓ Training samples: 5000\n✓ Test samples: 3638\n✓ Metric embeddings shape: (145, 768)\n✓ Unique metrics: 145\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"print(\"\\nANALYZING SCORE DISTRIBUTION\")\nprint(\"=\"*80)\n# Convert scores to numeric\ntrain_df['score'] = pd.to_numeric(train_df['score'])\n\nscore_counts = Counter(train_df['score'])\nprint(\"\\nScore Distribution:\")\nfor score in sorted(score_counts.keys()):\n    count = score_counts[score]\n    percentage = (count / len(train_df)) * 100\n    bar = '█' * int(percentage / 2)\n    print(f\"Score {int(score):2d}: {count:5d} ({percentage:5.1f}%) {bar}\")\n\nprint(f\"\\nMean: {train_df['score'].mean():.2f}\")\nprint(f\"Median: {train_df['score'].median():.2f}\")\nprint(f\"Std: {train_df['score'].std():.2f}\")\n\n# Create sample weights to handle skew\nscore_weights = len(train_df) / (len(score_counts) * train_df['score'].value_counts())\ntrain_df['sample_weight'] = train_df['score'].map(score_weights)\n\nprint(f\"\\n✓ Sample weights calculated (min: {train_df['sample_weight'].min():.3f}, max: {train_df['sample_weight'].max():.3f})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T13:04:26.834912Z","iopub.execute_input":"2025-11-21T13:04:26.835739Z","iopub.status.idle":"2025-11-21T13:04:26.848336Z","shell.execute_reply.started":"2025-11-21T13:04:26.835713Z","shell.execute_reply":"2025-11-21T13:04:26.847536Z"}},"outputs":[{"name":"stdout","text":"\nANALYZING SCORE DISTRIBUTION\n================================================================================\n\nScore Distribution:\nScore  0:    13 (  0.3%) \nScore  1:     6 (  0.1%) \nScore  2:     5 (  0.1%) \nScore  3:     7 (  0.1%) \nScore  4:     3 (  0.1%) \nScore  5:     1 (  0.0%) \nScore  6:    45 (  0.9%) \nScore  7:    95 (  1.9%) \nScore  8:   259 (  5.2%) ██\nScore  9:  3123 ( 62.5%) ███████████████████████████████\nScore  9:     1 (  0.0%) \nScore 10:  1442 ( 28.8%) ██████████████\n\nMean: 9.12\nMedian: 9.00\nStd: 0.94\n\n✓ Sample weights calculated (min: 0.133, max: 416.667)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"print(\"CREATING EMBEDDINGS WITH l3cube-pune/indic-sentence-similarity-sbert\")\nprint(\"=\"*80)\n\n# Load Indic Sentence Similarity model\nprint(\"\\nl3cube-pune/indic-sentence-similarity-sbert model\")\nembedding_model = SentenceTransformer(\n    \"l3cube-pune/indic-sentence-similarity-sbert\"\n)\n\nprint(\"✓ Model loaded successfully\")\nprint(f\"  Model dimension: {embedding_model.get_sentence_embedding_dimension()}\")\nprint(f\"  Max sequence length: {embedding_model.max_seq_length}\")\n\ndef encode_long_text_with_sliding_window(\n    texts,\n    model,\n    max_length=512,\n    stride=256,\n    batch_size=32,\n    show_progress=True,\n    normalize=True\n):\n\n    from tqdm.auto import tqdm\n    import torch\n\n    tokenizer = model.tokenizer\n    all_embeddings = []\n\n    texts_iter = tqdm(texts, desc=\"Encoding texts\") if show_progress else texts\n\n    for idx, text in enumerate(texts_iter):\n        try:\n            if text is None or (isinstance(text, float) and np.isnan(text)):\n                text = \"\"\n            elif not isinstance(text, str):\n                text = str(text)\n                \n            text = text.strip()\n\n            if len(text) == 0:\n                # Create zero embedding for empty text\n                embedding_dim = model.get_sentence_embedding_dimension()\n                embedding = np.zeros(embedding_dim)\n                all_embeddings.append(embedding)\n                continue\n\n            # Tokenize the full text with truncation disabled\n            tokens = tokenizer.encode(\n                text,\n                add_special_tokens=False,\n                truncation=False\n            )\n\n            # If text is short enough, encode directly\n            if len(tokens) <= max_length - 2:  \n                embedding = model.encode(\n                    [text],\n                    batch_size=1,\n                    show_progress_bar=False,\n                    convert_to_numpy=True,\n                    normalize_embeddings=normalize\n                )[0]\n                all_embeddings.append(embedding)\n                continue\n\n            # For long texts, use sliding window\n            chunk_embeddings = []\n            start_idx = 0\n\n            while start_idx < len(tokens):\n                # Extract chunk of tokens\n                end_idx = min(start_idx + max_length - 2, len(tokens))\n                chunk_tokens = tokens[start_idx:end_idx]\n\n                # Decode back to text\n                chunk_text = tokenizer.decode(chunk_tokens, skip_special_tokens=True)\n\n                # Encode chunk\n                chunk_emb = model.encode(\n                    [chunk_text],\n                    batch_size=1,\n                    show_progress_bar=False,\n                    convert_to_numpy=True,\n                    normalize_embeddings=False  \n                )[0]\n\n                chunk_embeddings.append(chunk_emb)\n\n                # Move window\n                if end_idx == len(tokens):\n                    break\n                start_idx += stride\n\n            # Average all chunk embeddings\n            if len(chunk_embeddings) > 0:\n                final_embedding = np.mean(chunk_embeddings, axis=0)\n\n                # Normalize if requested\n                if normalize:\n                    norm = np.linalg.norm(final_embedding)\n                    if norm > 0:\n                        final_embedding = final_embedding / norm\n\n                all_embeddings.append(final_embedding)\n            else:\n                # Fallback- create zero embedding\n                embedding_dim = model.get_sentence_embedding_dimension()\n                all_embeddings.append(np.zeros(embedding_dim))\n\n        except Exception as e:\n            print(f\"\\n⚠️  Error encoding text at index {idx}: {str(e)}\")\n            print(f\"   Text preview: {str(text)[:100]}...\")\n            # Create zero embedding as fallback\n            embedding_dim = model.get_sentence_embedding_dimension()\n            all_embeddings.append(np.zeros(embedding_dim))\n            continue\n\n    return np.array(all_embeddings)\n\n# Handle NULL values\ntrain_df['system_prompt'] = train_df['system_prompt'].fillna(\"\").astype(str)\ntest_df['system_prompt'] = test_df['system_prompt'].fillna(\"\").astype(str)\n\nif 'user_prompt' in train_df.columns:\n    train_df['prompt'] = train_df['user_prompt'].fillna(\"\").astype(str)\n    test_df['prompt'] = test_df['user_prompt'].fillna(\"\").astype(str)\nelse:\n    train_df['prompt'] = train_df['prompt'].fillna(\"\").astype(str)\n    test_df['prompt'] = test_df['prompt'].fillna(\"\").astype(str)\n\ntrain_df['response'] = train_df['response'].fillna(\"\").astype(str)\ntest_df['response'] = test_df['response'].fillna(\"\").astype(str)\n\n# ENCODE TRAINING DATA WITH SLIDING WINDOW\n\nprint(\"\\nEncoding training data with sliding window\")\nprint(\"Prompts\")\ntrain_prompt_embp = encode_long_text_with_sliding_window(\n    train_df['prompt'].tolist(),\n    embedding_model,\n    max_length=510,  # Slightly reduced to account for special tokens\n    stride=256,\n    batch_size=32,\n    show_progress=True,\n    normalize=True\n)\n\nprint(\"System prompts\")\ntrain_system_embp = encode_long_text_with_sliding_window(\n    train_df['system_prompt'].tolist(),\n    embedding_model,\n    max_length=510,\n    stride=256,\n    batch_size=32,\n    show_progress=True,\n    normalize=True\n)\n\nprint(\"Responses\")\ntrain_response_embp = encode_long_text_with_sliding_window(\n    train_df['response'].tolist(),\n    embedding_model,\n    max_length=510,\n    stride=256,\n    batch_size=32,\n    show_progress=True,\n    normalize=True\n)\n\nprint(\"\\nEncoding test data with sliding window\")\nprint(\"Prompts\")\ntest_prompt_embp = encode_long_text_with_sliding_window(\n    test_df['prompt'].tolist(),\n    embedding_model,\n    max_length=510,\n    stride=256,\n    batch_size=32,\n    show_progress=True,\n    normalize=True\n)\n\nprint(\"System prompts\")\ntest_system_embp = encode_long_text_with_sliding_window(\n    test_df['system_prompt'].tolist(),\n    embedding_model,\n    max_length=510,\n    stride=256,\n    batch_size=32,\n    show_progress=True,\n    normalize=True\n)\n\nprint(\"Responses\")\ntest_response_embp = encode_long_text_with_sliding_window(\n    test_df['response'].tolist(),\n    embedding_model,\n    max_length=510,\n    stride=256,\n    batch_size=32,\n    show_progress=True,\n    normalize=True\n)\n\nnp.save(output_dir / 'train_prompt_embp.npy', train_prompt_embp)\nnp.save(output_dir / 'train_system_embp.npy', train_system_embp)\nnp.save(output_dir / 'train_response_embp.npy', train_response_embp)\nnp.save(output_dir / 'test_prompt_embp.npy', test_prompt_embp)\nnp.save(output_dir / 'test_system_embp.npy', test_system_embp)\nnp.save(output_dir / 'test_response_embp.npy', test_response_embp)\n\nprint(f\"\\n✓ All embeddings saved to {output_dir}\")\nprint(f\"\\nEmbedding Shapes:\")\nprint(f\"  Train prompt: {train_prompt_embp.shape}\")\nprint(f\"  Train system: {train_system_embp.shape}\")\nprint(f\"  Train response: {train_response_embp.shape}\")\nprint(f\"  Test prompt: {test_prompt_embp.shape}\")\nprint(f\"  Test system: {test_system_embp.shape}\")\nprint(f\"  Test response: {test_response_embp.shape}\")\n\nprint(\"\\nEMBEDDING GENERATION COMPLETE\")\nprint(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T13:04:35.523856Z","iopub.execute_input":"2025-11-21T13:04:35.524401Z","iopub.status.idle":"2025-11-21T13:09:36.601768Z","shell.execute_reply.started":"2025-11-21T13:04:35.524365Z","shell.execute_reply":"2025-11-21T13:09:36.601111Z"}},"outputs":[{"name":"stdout","text":"CREATING EMBEDDINGS WITH l3cube-pune/indic-sentence-similarity-sbert\n================================================================================\n\nl3cube-pune/indic-sentence-similarity-sbert model\n✓ Model loaded successfully\n  Model dimension: 768\n  Max sequence length: 512\n\nEncoding training data with sliding window\nPrompts\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Encoding texts:   0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e11b6c0cc257414c97cce5fe0758f274"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (726 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"System prompts\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Encoding texts:   0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31e2ecb908b44ce88653c0438c0f48e7"}},"metadata":{}},{"name":"stdout","text":"Responses\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Encoding texts:   0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22f9d0bbfe1447b5817905cb1eadc7cb"}},"metadata":{}},{"name":"stdout","text":"\nEncoding test data with sliding window\nPrompts\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Encoding texts:   0%|          | 0/3638 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8d1caa7e6c74d16820213370ec32db8"}},"metadata":{}},{"name":"stdout","text":"System prompts\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Encoding texts:   0%|          | 0/3638 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cc5e8606e5d4caab842fd28f5f6ce02"}},"metadata":{}},{"name":"stdout","text":"Responses\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Encoding texts:   0%|          | 0/3638 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eef335805f884b57be39d5f98bb61a62"}},"metadata":{}},{"name":"stdout","text":"\n✓ All embeddings saved to /kaggle/working/output\n\nEmbedding Shapes:\n  Train prompt: (5000, 768)\n  Train system: (5000, 768)\n  Train response: (5000, 768)\n  Test prompt: (3638, 768)\n  Test system: (3638, 768)\n  Test response: (3638, 768)\n\nEMBEDDING GENERATION COMPLETE\n================================================================================\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error\nfrom collections import defaultdict\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\nimport xgboost as xgb\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"ORDINAL + XGBOOST ENSEMBLE\")\nprint(\"=\"*80)\n\ntrain_prompt_emb = np.load('/kaggle/input/puneemb/train_prompt_embp.npy')\ntrain_system_emb = np.load('/kaggle/input/puneemb/train_system_embp.npy')\ntrain_response_emb = np.load('/kaggle/input/puneemb/train_response_embp.npy')\ntest_prompt_emb = np.load('/kaggle/input/puneemb/test_prompt_embp.npy')\ntest_system_emb = np.load('/kaggle/input/puneemb/test_system_embp.npy')\ntest_response_emb = np.load('/kaggle/input/puneemb/test_response_embp.npy')\n\nmetric_embeddings_norm = metric_embeddings / np.linalg.norm(metric_embeddings, axis=1, keepdims=True)\nmetric_to_idx = {name: idx for idx, name in enumerate(metric_names_list)}\n\nprint(\"\\nDATA AUGMENTATION\")\nprint(\"=\"*80)\n\ndef generate_comprehensive_negatives(df, prompt_emb, system_emb, response_emb,\n                                     num_negatives_per_positive):\n    metric_groups = defaultdict(list)\n    for idx, row in df.iterrows():\n        metric_groups[row['metric_name']].append(idx)\n\n    augmented_data = []\n    augmented_prompt_emb = []\n    augmented_system_emb = []\n    augmented_response_emb = []\n\n    for idx, row in df.iterrows():\n        score = row['score']\n\n        if score <= 3:\n            replication_factor = 20\n        elif score <= 5:\n            replication_factor = 10\n        elif score <= 7:\n            replication_factor = 3\n        else:\n            replication_factor = 1\n\n        for _ in range(replication_factor):\n            augmented_data.append({\n                'metric_name': row['metric_name'],\n                'prompt': row['prompt'],\n                'system_prompt': row['system_prompt'],\n                'response': row['response'],\n                'score': row['score'],\n                'is_synthetic': False,\n                'original_idx': idx\n            })\n            augmented_prompt_emb.append(prompt_emb[idx])\n            augmented_system_emb.append(system_emb[idx])\n            augmented_response_emb.append(response_emb[idx])\n\n    high_score_indices = df[df['score'] >= 7].index.tolist()\n\n    for high_idx in high_score_indices:\n        high_row = df.loc[high_idx]\n        high_metric = high_row['metric_name']\n\n        other_metrics = [m for m in metric_groups.keys() if m != high_metric]\n        if len(other_metrics) == 0:\n            continue\n\n        for _ in range(num_negatives_per_positive):\n            neg_metric = np.random.choice(other_metrics)\n            neg_idx = np.random.choice(metric_groups[neg_metric])\n            neg_row = df.loc[neg_idx]\n\n            synthetic_score = np.random.randint(0, 3)\n\n            augmented_data.append({\n                'metric_name': high_metric,\n                'prompt': neg_row['prompt'],\n                'system_prompt': neg_row['system_prompt'],\n                'response': neg_row['response'],\n                'score': synthetic_score,\n                'is_synthetic': True,\n                'original_idx': high_idx\n            })\n\n            augmented_prompt_emb.append(prompt_emb[neg_idx])\n            augmented_system_emb.append(system_emb[neg_idx])\n            augmented_response_emb.append(response_emb[neg_idx])\n\n    augmented_df = pd.DataFrame(augmented_data)\n\n    print(f\"Augmented: {len(augmented_df)} samples\")\n    print(f\"  Synthetic: {augmented_df['is_synthetic'].sum()}\")\n\n    return (augmented_df,\n            np.array(augmented_prompt_emb),\n            np.array(augmented_system_emb),\n            np.array(augmented_response_emb))\n\ntrain_aug_df, train_aug_prompt_emb, train_aug_system_emb, train_aug_response_emb = \\\n    generate_comprehensive_negatives(train_df, train_prompt_emb, train_system_emb,\n                                    train_response_emb, num_negatives_per_positive=13)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T13:10:14.688852Z","iopub.execute_input":"2025-11-21T13:10:14.689176Z","iopub.status.idle":"2025-11-21T13:10:25.252771Z","shell.execute_reply.started":"2025-11-21T13:10:14.689152Z","shell.execute_reply":"2025-11-21T13:10:25.252117Z"}},"outputs":[{"name":"stdout","text":"ORDINAL + XGBOOST ENSEMBLE\n================================================================================\n\nDATA AUGMENTATION\n================================================================================\nAugmented: 69865 samples\n  Synthetic: 63960\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"print(\"ORDINAL REGRESSION MODEL\")\nprint(\"=\"*80)\n\nclass OrdinalRegressionDataset(Dataset):\n    def __init__(self, metric_emb, prompt_emb, system_emb, response_emb, scores, weights=None):\n        self.metric_emb = torch.FloatTensor(metric_emb)\n        self.prompt_emb = torch.FloatTensor(prompt_emb)\n        self.system_emb = torch.FloatTensor(system_emb)\n        self.response_emb = torch.FloatTensor(response_emb)\n        self.scores = torch.LongTensor(scores.astype(int))\n        self.weights = torch.FloatTensor(weights) if weights is not None else torch.ones(len(scores))\n\n    def __len__(self):\n        return len(self.scores)\n\n    def __getitem__(self, idx):\n        return (self.metric_emb[idx], self.prompt_emb[idx],\n                self.system_emb[idx], self.response_emb[idx],\n                self.scores[idx], self.weights[idx])\n\nclass OrdinalWithThresholds(nn.Module):\n    def __init__(self, input_dim=768, num_classes=11):\n        super().__init__()\n\n        self.feature_net = nn.Sequential(\n            nn.Linear(input_dim * 4 + 10, 512),\n            nn.LayerNorm(512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.LayerNorm(256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 1)\n        )\n\n        initial_thresholds = torch.linspace(0.5, 9.5, num_classes - 1)\n        self.thresholds = nn.Parameter(initial_thresholds)\n\n    def forward(self, metric_emb, prompt_emb, system_emb, response_emb):\n        sims = torch.stack([\n            (metric_emb * prompt_emb).sum(dim=1),\n            (metric_emb * system_emb).sum(dim=1),\n            (metric_emb * response_emb).sum(dim=1),\n            (prompt_emb * response_emb).sum(dim=1),\n            (prompt_emb * system_emb).sum(dim=1),\n            (system_emb * response_emb).sum(dim=1),\n            (metric_emb * (prompt_emb + response_emb) / 2).sum(dim=1),\n            (metric_emb * (prompt_emb + system_emb + response_emb) / 3).sum(dim=1),\n            ((prompt_emb + response_emb) * metric_emb).sum(dim=1),\n            ((system_emb + response_emb) * metric_emb).sum(dim=1)\n        ], dim=1)\n\n        combined = torch.cat([metric_emb, prompt_emb, system_emb, response_emb, sims], dim=1)\n        continuous_score = self.feature_net(combined).squeeze()\n\n        return continuous_score\n\n    def get_class_probabilities(self, continuous_score):\n        sorted_thresholds = torch.sort(self.thresholds)[0]\n        cumulative_probs = torch.sigmoid(continuous_score.unsqueeze(1) - sorted_thresholds.unsqueeze(0))\n\n        num_classes = len(sorted_thresholds) + 1\n        class_probs = torch.zeros(continuous_score.size(0), num_classes, device=continuous_score.device)\n\n        class_probs[:, 0] = 1 - cumulative_probs[:, 0]\n        for i in range(1, num_classes - 1):\n            class_probs[:, i] = cumulative_probs[:, i-1] - cumulative_probs[:, i]\n        class_probs[:, -1] = cumulative_probs[:, -1]\n\n        return class_probs\n\ndef ordinal_loss_with_weights(continuous_scores, targets, thresholds, weights):\n    sorted_thresholds = torch.sort(thresholds)[0]\n    batch_size = continuous_scores.size(0)\n    num_thresholds = len(sorted_thresholds)\n\n    cumulative_labels = torch.zeros(batch_size, num_thresholds, device=continuous_scores.device)\n    for i, target in enumerate(targets):\n        cumulative_labels[i, :target] = 1\n\n    cumulative_probs = torch.sigmoid(continuous_scores.unsqueeze(1) - sorted_thresholds.unsqueeze(0))\n    bce = F.binary_cross_entropy(cumulative_probs, cumulative_labels, reduction='none')\n    weighted_bce = bce * weights.unsqueeze(1)\n\n    return weighted_bce.mean()\n\ndef train_ordinal_model(train_dataset, val_dataset, epochs=25):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model = OrdinalWithThresholds().to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0005, weight_decay=0.01)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n\n    train_scores = train_dataset.scores.numpy()\n    score_counts = np.bincount(train_scores, minlength=11)\n    score_weights = 1.0 / (score_counts + 1)\n    sample_weights = score_weights[train_scores]\n    sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n\n    train_loader = DataLoader(train_dataset, batch_size=128, sampler=sampler)\n    val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n\n    best_rmse = float('inf')\n    best_model = None\n    patience_counter = 0\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0\n\n        for batch in train_loader:\n            metric_emb, prompt_emb, system_emb, response_emb, targets, weights = [b.to(device) for b in batch]\n\n            optimizer.zero_grad()\n            continuous_scores = model(metric_emb, prompt_emb, system_emb, response_emb)\n            loss = ordinal_loss_with_weights(continuous_scores, targets, model.thresholds, weights)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n            train_loss += loss.item()\n\n        model.eval()\n        val_preds = []\n        val_targets = []\n\n        with torch.no_grad():\n            for batch in val_loader:\n                metric_emb, prompt_emb, system_emb, response_emb, targets, _ = [b.to(device) for b in batch]\n                continuous_scores = model(metric_emb, prompt_emb, system_emb, response_emb)\n                class_probs = model.get_class_probabilities(continuous_scores)\n                pred_scores = (class_probs * torch.arange(11, device=device).float()).sum(dim=1)\n\n                val_preds.extend(pred_scores.cpu().numpy())\n                val_targets.extend(targets.cpu().numpy())\n\n        val_preds = np.clip(np.round(val_preds), 0, 10)\n        val_rmse = np.sqrt(mean_squared_error(val_targets, val_preds))\n\n        if val_rmse < best_rmse:\n            best_rmse = val_rmse\n            best_model = model.state_dict()\n            patience_counter = 0\n        else:\n            patience_counter += 1\n            if patience_counter >= 10:\n                break\n\n        scheduler.step()\n\n    model.load_state_dict(best_model)\n    return model\n\n# Prepare data\ntrain_aug_metric_embs = np.array([\n    metric_embeddings_norm[metric_to_idx[row['metric_name']]]\n    if row['metric_name'] in metric_to_idx\n    else np.mean(metric_embeddings_norm, axis=0)\n    for _, row in train_aug_df.iterrows()\n])\n\ntest_metric_embs = np.array([\n    metric_embeddings_norm[metric_to_idx[row['metric_name']]]\n    if row['metric_name'] in metric_to_idx\n    else np.mean(metric_embeddings_norm, axis=0)\n    for _, row in test_df.iterrows()\n])\n\nscore_counts = train_aug_df['score'].value_counts()\nmax_count = score_counts.max()\nsample_weights = train_aug_df['score'].map(lambda x: max_count / score_counts[x]).values\n\n# CV setup\ngkf = GroupKFold(n_splits=5)\noriginal_groups = np.array([\n    train_df.loc[row['original_idx'], 'metric_name'] if not row['is_synthetic']\n    else f\"synthetic_{row['original_idx']}\"\n    for _, row in train_aug_df.iterrows()\n])\n\nordinal_models = []\nordinal_oof = np.zeros(len(train_df))\nordinal_test = np.zeros(len(test_df))\n\n# Train ordinal models\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(train_aug_metric_embs,\n                                                       train_aug_df['score'],\n                                                       original_groups), 1):\n    print(f\"\\nOrdinal Fold {fold}/5\")\n\n    val_original_mask = ~train_aug_df.iloc[val_idx]['is_synthetic'].values\n    val_original_indices = train_aug_df.iloc[val_idx][val_original_mask]['original_idx'].values\n\n    train_dataset = OrdinalRegressionDataset(\n        train_aug_metric_embs[train_idx], train_aug_prompt_emb[train_idx],\n        train_aug_system_emb[train_idx], train_aug_response_emb[train_idx],\n        train_aug_df['score'].values[train_idx], sample_weights[train_idx]\n    )\n\n    val_dataset = OrdinalRegressionDataset(\n        train_aug_metric_embs[val_idx][val_original_mask],\n        train_aug_prompt_emb[val_idx][val_original_mask],\n        train_aug_system_emb[val_idx][val_original_mask],\n        train_aug_response_emb[val_idx][val_original_mask],\n        train_aug_df['score'].values[val_idx][val_original_mask],\n        sample_weights[val_idx][val_original_mask]\n    )\n\n    model = train_ordinal_model(train_dataset, val_dataset, epochs=25)\n    ordinal_models.append(model)\n\n    # OOF\n    model.eval()\n    device = next(model.parameters()).device\n    val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n\n    val_preds = []\n    with torch.no_grad():\n        for batch in val_loader:\n            metric_emb, prompt_emb, system_emb, response_emb, _, _ = [b.to(device) for b in batch]\n            continuous_scores = model(metric_emb, prompt_emb, system_emb, response_emb)\n            class_probs = model.get_class_probabilities(continuous_scores)\n            pred_scores = (class_probs * torch.arange(11, device=device).float()).sum(dim=1)\n            val_preds.extend(pred_scores.cpu().numpy())\n\n    ordinal_oof[val_original_indices] = np.array(val_preds)\n\n    # Test\n    test_dataset = OrdinalRegressionDataset(\n        test_metric_embs, test_prompt_emb, test_system_emb, test_response_emb,\n        np.zeros(len(test_df)), np.ones(len(test_df))\n    )\n    test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n\n    test_preds = []\n    with torch.no_grad():\n        for batch in test_loader:\n            metric_emb, prompt_emb, system_emb, response_emb, _, _ = [b.to(device) for b in batch]\n            continuous_scores = model(metric_emb, prompt_emb, system_emb, response_emb)\n            class_probs = model.get_class_probabilities(continuous_scores)\n            pred_scores = (class_probs * torch.arange(11, device=device).float()).sum(dim=1)\n            test_preds.extend(pred_scores.cpu().numpy())\n\n    ordinal_test += np.array(test_preds) / 5\n\nordinal_oof_rounded = np.clip(np.round(ordinal_oof), 0, 10)\nordinal_rmse = np.sqrt(mean_squared_error(train_df['score'], ordinal_oof_rounded))\nprint(f\"\\n✓ Ordinal OOF RMSE: {ordinal_rmse:.4f}\")\n\n# ADD XGBOOST ENSEMBLE\n\nprint(\"\\nADDING XGBOOST ENSEMBLE\")\nprint(\"=\"*80)\n\n# Create simple features for XGBoost\nX_train_aug = np.concatenate([\n    train_aug_metric_embs,\n    train_aug_prompt_emb,\n    train_aug_response_emb,\n    (train_aug_metric_embs * train_aug_prompt_emb),\n    (train_aug_metric_embs * train_aug_response_emb),\n    (train_aug_prompt_emb * train_aug_response_emb)\n], axis=1)\n\nX_test = np.concatenate([\n    test_metric_embs,\n    test_prompt_emb,\n    test_response_emb,\n    (test_metric_embs * test_prompt_emb),\n    (test_metric_embs * test_response_emb),\n    (test_prompt_emb * test_response_emb)\n], axis=1)\n\nxgb_oof = np.zeros(len(train_df))\nxgb_test = np.zeros(len(test_df))\n\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(train_aug_metric_embs,\n                                                       train_aug_df['score'],\n                                                       original_groups), 1):\n    print(f\"XGBoost Fold {fold}/5\", end=' ')\n\n    val_original_mask = ~train_aug_df.iloc[val_idx]['is_synthetic'].values\n    val_original_indices = train_aug_df.iloc[val_idx][val_original_mask]['original_idx'].values\n\n    dtrain = xgb.DMatrix(X_train_aug[train_idx],\n                        label=train_aug_df['score'].values[train_idx],\n                        weight=sample_weights[train_idx])\n    dval = xgb.DMatrix(X_train_aug[val_idx][val_original_mask],\n                      label=train_aug_df['score'].values[val_idx][val_original_mask])\n    dtest = xgb.DMatrix(X_test)\n\n    params = {\n        'objective': 'reg:squarederror',\n        'max_depth': 6,\n        'learning_rate': 0.02,\n        'subsample': 0.85,\n        'colsample_bytree': 0.85,\n        'reg_alpha': 0.1,\n        'reg_lambda': 1.5,\n        'random_state': 42\n    }\n\n    model = xgb.train(params, dtrain, num_boost_round=600,\n                     evals=[(dval, 'val')], early_stopping_rounds=50, verbose_eval=False)\n\n    xgb_oof[val_original_indices] = model.predict(dval)\n    xgb_test += model.predict(dtest) / 5\n\n    rmse = np.sqrt(mean_squared_error(\n        train_df.loc[val_original_indices, 'score'],\n        np.clip(np.round(model.predict(dval)), 0, 10)\n    ))\n    print(f\"RMSE: {rmse:.4f}\")\n\nxgb_oof_rounded = np.clip(np.round(xgb_oof), 0, 10)\nxgb_rmse = np.sqrt(mean_squared_error(train_df['score'], xgb_oof_rounded))\nprint(f\"✓ XGBoost OOF RMSE: {xgb_rmse:.4f}\")\n\n# OPTIMAL ENSEMBLE WEIGHTING\n\nprint(\"\\nFINDING OPTIMAL ENSEMBLE WEIGHT\")\nprint(\"=\"*80)\n\nbest_rmse = float('inf')\nbest_weight = 0.5\n\nfor ordinal_weight in np.arange(0.5, 1.0, 0.05):\n    xgb_weight = 1 - ordinal_weight\n    ensemble_oof = ordinal_weight * ordinal_oof + xgb_weight * xgb_oof\n    ensemble_oof_rounded = np.clip(np.round(ensemble_oof), 0, 10)\n    rmse = np.sqrt(mean_squared_error(train_df['score'], ensemble_oof_rounded))\n\n    if rmse < best_rmse:\n        best_rmse = rmse\n        best_weight = ordinal_weight\n\nprint(f\"Optimal: {best_weight:.2f} Ordinal + {1-best_weight:.2f} XGBoost\")\nprint(f\"Ensemble OOF RMSE: {best_rmse:.4f}\")\nprint(f\"Improvement: {ordinal_rmse - best_rmse:.4f}\")\n\n# Final predictions\nfinal_test = best_weight * ordinal_test + (1 - best_weight) * xgb_test\nfinal_test = np.clip(np.round(final_test), 0, 10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T13:11:59.989830Z","iopub.execute_input":"2025-11-21T13:11:59.990741Z","iopub.status.idle":"2025-11-21T13:23:11.186679Z","shell.execute_reply.started":"2025-11-21T13:11:59.990709Z","shell.execute_reply":"2025-11-21T13:23:11.185844Z"}},"outputs":[{"name":"stdout","text":"ORDINAL REGRESSION MODEL\n================================================================================\n\nOrdinal Fold 1/5\n\nOrdinal Fold 2/5\n\nOrdinal Fold 3/5\n\nOrdinal Fold 4/5\n\nOrdinal Fold 5/5\n\n✓ Ordinal OOF RMSE: 4.0077\n\nADDING XGBOOST ENSEMBLE\n================================================================================\nXGBoost Fold 1/5 RMSE: 5.5887\nXGBoost Fold 2/5 RMSE: 4.9165\nXGBoost Fold 3/5 RMSE: 4.9857\nXGBoost Fold 4/5 RMSE: 4.7968\nXGBoost Fold 5/5 RMSE: 5.3610\n✓ XGBoost OOF RMSE: 5.4601\n\nFINDING OPTIMAL ENSEMBLE WEIGHT\n================================================================================\nOptimal: 0.95 Ordinal + 0.05 XGBoost\nEnsemble OOF RMSE: 4.0262\nImprovement: -0.0185\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"submission = sample_submission.copy()\nsubmission['score'] = final_test\nsubmission.to_csv('/kaggle/working/output/submission_ensemble.csv', index=False)\n\nprint(\"\\n✓ Submission created\")\nprint(f\"\\nTest mean: {submission['score'].mean():.2f}\")\nprint(f\"\\nDistribution:\")\nfor score in sorted(submission['score'].unique()):\n    count = (submission['score'] == score).sum()\n    pct = count / len(submission) * 100\n    bar = '█' * int(pct / 2)\n    print(f\"Score {int(score):2d}: {count:4d} ({pct:5.1f}%) {bar}\")\n\nprint(\"\\nCOMPLETE\")\nprint(\"=\"*80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T13:27:09.034555Z","iopub.execute_input":"2025-11-21T13:27:09.035149Z","iopub.status.idle":"2025-11-21T13:27:09.049871Z","shell.execute_reply.started":"2025-11-21T13:27:09.035127Z","shell.execute_reply":"2025-11-21T13:27:09.049072Z"}},"outputs":[{"name":"stdout","text":"\n✓ Submission created\n\nTest mean: 6.20\n\nDistribution:\nScore  0:   79 (  2.2%) █\nScore  1:  337 (  9.3%) ████\nScore  2:  209 (  5.7%) ██\nScore  3:  164 (  4.5%) ██\nScore  4:  150 (  4.1%) ██\nScore  5:  152 (  4.2%) ██\nScore  6:  261 (  7.2%) ███\nScore  7:  633 ( 17.4%) ████████\nScore  8:  975 ( 26.8%) █████████████\nScore  9:  613 ( 16.8%) ████████\nScore 10:   65 (  1.8%) \n\nCOMPLETE\n================================================================================\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}